{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7299f6",
   "metadata": {},
   "source": [
    "# E-Commerce Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2f755",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "This notebook demonstrates the development of a **product recommendation system** for an **e-commerce dataset**.  \n",
    "It utilizes the [`implicit`](https://github.com/benfred/implicit) library for building an **Alternating Least Squares (ALS)** model, a popular collaborative filtering algorithm.  \n",
    "\n",
    "The notebook covers the following steps:\n",
    "\n",
    "- **Data Loading and Preprocessing**  \n",
    "- **Exploratory Data Analysis (EDA)**  \n",
    "- **Data Splitting** for training and testing  \n",
    "- **Model Training** using the ALS algorithm  \n",
    "- **Model Evaluation** using various ranking metrics:\n",
    "  - Precision@k  \n",
    "  - Recall@k  \n",
    "  - Mean Average Precision (MAP@k)  \n",
    "  - Normalized Discounted Cumulative Gain (NDCG@k)  \n",
    "\n",
    "Finally, it showcases how to **generate product recommendations** for individual customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5cabf",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcd285",
   "metadata": {},
   "source": [
    "##### This section imports necessary libraries and loads the e-commerce dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf139a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import implicit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils.functions import ModelSolutions\n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas option to display all columns in a DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Model Solution Initialization\n",
    "model_solutions = ModelSolutions()\n",
    "\n",
    "# Load the E-Commerce data from a CSV file\n",
    "df = model_solutions.load_data(folders=['data','raw'],file_name='E-Commerce_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c38088",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df833723",
   "metadata": {},
   "source": [
    "##### This section cleans and prepares the data for model training. It involves converting date columns, handling missing values, and filtering out returned/exchanged items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22946f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime objects\n",
    "df[\"purchase_date\"] = pd.to_datetime(df[\"purchase_date\"])\n",
    "df[\"return_date\"] = pd.to_datetime(df[\"return_date\"], errors=\"coerce\")\n",
    "\n",
    "# Including Related Columns for the model\n",
    "model_data = df[\n",
    "    ['id','returning','product','category','purchase_amount','purchase_date','returned','return_date']\n",
    "    ].copy()\n",
    "\n",
    "# Filling missing values in 'return_date'\n",
    "model_data['return_date'] = model_data['return_date'].fillna(\"Not-Returned\")\n",
    "\n",
    "# Filter out returned or exchanged items to focus on successful purchases\n",
    "fd = model_data[(model_data['returned']!='refund') & (model_data['returned']!='exchange')]\n",
    "\n",
    "# Prepare data for collaborative filtering, focusing on 'id', 'product', and 'purchase_amount'\n",
    "cf_data = fd[['id','product', 'purchase_amount']].copy()\n",
    "\n",
    "# Identify users with high purchase counts (more than 4 purchases)\n",
    "purchase_counts = cf_data.groupby('id').size()\n",
    "high_purchase_ids = purchase_counts[purchase_counts > 4].index\n",
    "filtered_cf = cf_data[cf_data['id'].isin(high_purchase_ids)]\n",
    "\n",
    "# Aggregate low purchase data (users with 4 or fewer purchases) by summing purchase amounts\n",
    "low_purchase_data = cf_data[~cf_data['id'].isin(high_purchase_ids)]\n",
    "low_purchase_data = low_purchase_data.groupby(['id','product'])['purchase_amount'].sum().reset_index()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split high purchase data into training and testing sets based on a random mask\n",
    "# Approximately 80% for training\n",
    "split_mask = filtered_cf.groupby('id')['purchase_amount'].transform(lambda x: np.random.rand(len(x))<0.8)\n",
    "train_data = filtered_cf[split_mask]\n",
    "test_data = filtered_cf[~split_mask]\n",
    "\n",
    "# Create a pivot table for the complete dataset, counting purchase amounts\n",
    "data = cf_data.groupby(['id','product'])['purchase_amount'].count().reset_index()\n",
    "\n",
    "# Create pivot tables for training and testing data, representing user-item interactions\n",
    "train_pivot = train_data.pivot_table(index='id',columns='product',values='purchase_amount',aggfunc='count',fill_value=0)\n",
    "test_pivot = test_data.pivot_table(index='id',columns='product',values='purchase_amount',aggfunc='count',fill_value=0)\n",
    "data = data.pivot_table(index='id',columns='product',values='purchase_amount',aggfunc='count',fill_value=0)\n",
    "\n",
    "# Convert pivot tables to Compressed Sparse Row (CSR) matrices for efficient computation with implicit library\n",
    "train_csr = sparse.csr_matrix(train_pivot.values)\n",
    "test_csr = sparse.csr_matrix(test_pivot.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b4d00",
   "metadata": {},
   "source": [
    "### 3. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261386d",
   "metadata": {},
   "source": [
    "##### This section defines functions for evaluating the recommendation model using various metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19860e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_evaluation(test_csr, predictions, k):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations using average Precision@k and Recall@k.\n",
    "    \n",
    "    Parameters:\n",
    "      test_csr: 2D array or CSR matrix of actual user-item interactions (n_users x n_items).\n",
    "      predictions: 2D array where each row contains recommended item indices for the corresponding user.\n",
    "      k: Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "      avg_precision: Average Precision@k over all users in the predictions array.\n",
    "      avg_recall: Average Recall@k over all users in the predictions array.\n",
    "    \"\"\"\n",
    "    n_users = predictions.shape[0]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for user in range(n_users):\n",
    "        if hasattr(test_csr[user], \"toarray\"):\n",
    "            user_actual = test_csr[user].toarray()[0]\n",
    "        else:\n",
    "            user_actual = test_csr[user]\n",
    "        \n",
    "        actual_items = set(np.where(user_actual > 0)[0])\n",
    "        recommended_items = set(predictions[user][:k])\n",
    "        \n",
    "        hits = len(actual_items & recommended_items)\n",
    "        \n",
    "        precision = hits / k\n",
    "        recall = hits / len(actual_items) if actual_items else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    \n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "\n",
    "def evaluate_ranking_metrics(test_csr, predictions, k):\n",
    "    \"\"\"\n",
    "    Evaluate ranking metrics: Mean Average Precision (MAP@k) and NDCG@k.\n",
    "    Parameters:\n",
    "      test_csr: 2D array or CSR matrix of actual user-item interactions \n",
    "                (shape: n_users x n_items).\n",
    "      predictions: 2D array where each row contains recommended item indices \n",
    "                   for the corresponding user.\n",
    "      k: Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "      avg_map: Mean Average Precision at k over all users.\n",
    "      avg_ndcg: Mean Normalized Discounted Cumulative Gain at k over all users.\n",
    "    \"\"\"\n",
    "    map_scores = []\n",
    "    ndcg_scores = []\n",
    "    n_users = predictions.shape[0]\n",
    "    \n",
    "    for user in range(n_users):\n",
    "        if hasattr(test_csr[user], \"toarray\"):\n",
    "            user_actual = test_csr[user].toarray()[0]\n",
    "        else:\n",
    "            user_actual = test_csr[user]\n",
    "         \n",
    "        actual_items = set(np.where(user_actual > 0)[0])\n",
    "        if not actual_items:\n",
    "            continue\n",
    "        \n",
    "        # ----- MAP@k Calculation -----\n",
    "        num_hits = 0.0\n",
    "        ap = 0.0\n",
    "        for i, pred in enumerate(predictions[user][:k]):\n",
    "            if pred in actual_items:\n",
    "                num_hits += 1\n",
    "                ap += num_hits / (i + 1)\n",
    "        average_precision = ap / len(actual_items)\n",
    "        map_scores.append(average_precision)\n",
    "        \n",
    "        # ----- NDCG@k Calculation -----\n",
    "        dcg = 0.0\n",
    "        for i, pred in enumerate(predictions[user][:k]):\n",
    "            if pred in actual_items:\n",
    "                dcg += 1.0 / np.log2(i + 2)  # i+2 because positions are 1-indexed in the log term.\n",
    "        \n",
    "        ideal_hits = min(len(actual_items), k)\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    avg_map = np.mean(map_scores) if map_scores else 0.0\n",
    "    avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "    return avg_map, avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70271fea",
   "metadata": {},
   "source": [
    "### 4. Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0b243",
   "metadata": {},
   "source": [
    "##### This section trains the ALS model on the `train_csr` data and evaluates its performance on the `test_csr` using the defined metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e30453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.45it/s, loss=0.0155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@50: 0.0365\n",
      "NDCG@50: 0.1053\n",
      "Average Precision@50: 0.0130\n",
      "Average Recall@50: 0.3105\n"
     ]
    }
   ],
   "source": [
    "# Define the number of recommendations to generate for evaluation.\n",
    "N_recommendations = 50 \n",
    "\n",
    "# Initialize the Alternating Least Squares (ALS) model.\n",
    "# 'factors' determines the dimensionality of the latent factors (embedding size).\n",
    "# 'calculate_training_loss=True' enables calculation of loss during training, which can be useful for monitoring.\n",
    "model = implicit.als.AlternatingLeastSquares(factors=5, calculate_training_loss=True)\n",
    "\n",
    "# Train the ALS model on the training sparse matrix.\n",
    "# The 'implicit' library expects the user-item matrix to be in CSR format.\n",
    "model.fit(train_csr)\n",
    "\n",
    "# --- Evaluation Preparation: Aligning Predictions and Actuals ---\n",
    "# Identify common user IDs present in both training and test sets.\n",
    "# We can only evaluate for users who have interactions in both.\n",
    "common_user_ids = list(set(train_pivot.index) & set(test_pivot.index))\n",
    "\n",
    "# Initialize lists to store recommendations and actual interactions for common users.\n",
    "# These lists will be aligned, meaning the i-th element in aligned_predictions will correspond\n",
    "# to the i-th element in aligned_actuals_csr, both for the same user.\n",
    "aligned_predictions = []\n",
    "aligned_actuals_csr = []\n",
    "\n",
    "# Iterate through each common user to generate their recommendations and fetch their actual test interactions.\n",
    "for user_id in common_user_ids:\n",
    "    # Get the integer index of the user in the training pivot table.\n",
    "    # This index is used by the 'model.recommend' method.\n",
    "    train_user_idx = train_pivot.index.get_loc(user_id)\n",
    "    \n",
    "    # Get the integer index of the user in the test pivot table.\n",
    "    # This index is used to retrieve the actual interactions from 'test_csr'.\n",
    "    test_user_idx = test_pivot.index.get_loc(user_id)\n",
    "\n",
    "    # Generate recommendations for this specific user.\n",
    "    # 'user_items' provides the user's interactions from the training set to the model.\n",
    "    # 'filter_already_liked_items=True' ensures that the model doesn't recommend items the user already interacted with in training.\n",
    "    recommended_items_indices, _ = model.recommend(\n",
    "        userid=train_user_idx,\n",
    "        user_items=train_csr[train_user_idx],\n",
    "        N=N_recommendations,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "    aligned_predictions.append(recommended_items_indices)\n",
    "    \n",
    "    # Collect the actual interactions for this user from the test set.\n",
    "    aligned_actuals_csr.append(test_csr[test_user_idx])\n",
    "\n",
    "# Convert the list of recommendation arrays into a single NumPy array,\n",
    "# which is the expected format for the evaluation functions.\n",
    "aligned_predictions_np = np.array(aligned_predictions)\n",
    "\n",
    "# --- Evaluation ---\n",
    "# Define K, the number of top recommendations to consider for evaluation.\n",
    "K_eval = 50 \n",
    "\n",
    "# Call the evaluation functions with the now aligned predictions and actuals.\n",
    "avg_precision, avg_recall = simple_evaluation(aligned_actuals_csr, aligned_predictions_np, k=K_eval)\n",
    "avg_map, avg_ndcg = evaluate_ranking_metrics(aligned_actuals_csr, aligned_predictions_np, k=K_eval)\n",
    "\n",
    "# Print the evaluation results, formatted to four decimal places for readability.\n",
    "print(f\"MAP@{K_eval}: {avg_map:.4f}\")\n",
    "print(f\"NDCG@{K_eval}: {avg_ndcg:.4f}\")\n",
    "print(f\"Average Precision@{K_eval}: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall@{K_eval}: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06675de6",
   "metadata": {},
   "source": [
    "### 5. Generating Recommendations for a Specific Customer (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f6006",
   "metadata": {},
   "source": [
    "##### This section demonstrates how to get recommendations for a randomly chosen customer from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32beef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Car Wash Mitt', 'Frozen Meals', 'Dehumidifier',\n",
      "       'Headlight Restoration Kit', 'Dog Collar', 'Convection Oven',\n",
      "       'Cat Shampoo', 'Kombucha', 'Car Drying Towel', 'Bangle', 'Candy',\n",
      "       'Acne Treatment', 'Hand Cream', 'Hamster Cage', 'Wine', 'Spark Plugs',\n",
      "       'Tire Shine', 'Bread Maker', 'RC Helicopters', 'Utility Knife'],\n",
      "      dtype='object', name='product')\n"
     ]
    }
   ],
   "source": [
    "# Choose a random customer from the test set\n",
    "customer_id = np.random.choice(list(test_pivot.index))\n",
    "\n",
    "# Get recommendations for the selected customer\n",
    "# user_items is set to the specific row in test_csr for that customer\n",
    "product_indices, scores = model.recommend(test_pivot.index.get_loc(customer_id),\n",
    "                                          user_items= test_csr[test_pivot.index.get_loc(customer_id)],\n",
    "                                          N=20, # Number of recommendations to return\n",
    "                                          filter_already_liked_items=False # Do not filter already liked items in this case for demonstration\n",
    "                                          )\n",
    "\n",
    "# Display the recommended product names\n",
    "print(test_pivot.columns[product_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a4492",
   "metadata": {},
   "source": [
    "### 6. Training on Complete Data and Generating Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f46a60",
   "metadata": {},
   "source": [
    "This section shows how to train the model on the entire dataset (`data_csr`) and then generate recommendations for a random customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acdffc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:45<00:00,  3.04s/it, loss=0.0109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Stand Mixer', 'Smart Lock', 'Snare Drum', 'Beer', 'Toe Ring', 'Chips',\n",
      "       'Pet Scale', 'Travel Guides', 'Miter Saw', 'Thriller Novels'],\n",
      "      dtype='object', name='product')\n"
     ]
    }
   ],
   "source": [
    "# Convert the complete data pivot table to a CSR matrix\n",
    "data_csr = sparse.csc_matrix(data)\n",
    "\n",
    "# Train the ALS model on the complete dataset\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50, calculate_training_loss=True)\n",
    "model.fit(data_csr)\n",
    "\n",
    "# Choose a random customer from the complete dataset\n",
    "customer_id = np.random.choice(list(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb09271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Stand Mixer', 'Smart Lock', 'Snare Drum', 'Beer', 'Toe Ring', 'Chips',\n",
      "       'Pet Scale', 'Travel Guides', 'Miter Saw', 'Thriller Novels'],\n",
      "      dtype='object', name='product')\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations for the selected customer from the complete dataset\n",
    "product_indices, scores = model.recommend(data.index.get_loc(customer_id),\n",
    "                                          user_items= data_csr[data.index.get_loc(customer_id)].tocsr(),\n",
    "                                          N=10, # Number of recommendations\n",
    "                                          filter_already_liked_items=True # Filter out items the user has already interacted with\n",
    "                                          )\n",
    "\n",
    "# Display the recommended product names\n",
    "print(data.columns[product_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the model\n",
    "model_solutions.save_model(folders=['models','product_recommendation'],file_name='product_recommendation_model.pkl',obj=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecommerce_virtual_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
